\chapter{Implementazione}
\label{chp:implementazione}

In questo capitolo si descrive l'implementazione del sistema sperimentale per la
valutazione di algoritmi di stima di distinti su stream. L'architettura separa tre
responsabilita' principali:
\begin{itemize}
    \item logica algoritmica (\codepath{src/satp/algorithms});
    \item I/O del dataset binario partizionato (\codepath{src/satp/io/BinaryDatasetIO.h});
    \item valutazione, metriche ed export CSV (\codepath{src/satp/simulation}).
\end{itemize}

Il nucleo e' implementato in \texttt{C++} (CLI, framework e sketch), mentre
\texttt{Python} copre generazione dataset e orchestrazione batch
(\codepath{scripts/generate_partitioned_dataset_bin.py},
\codepath{scripts/orchestrate_benchmarks.py}).

\section{Architettura del sistema}

I moduli principali e le rispettive responsabilita' sono:
\begin{itemize}
    \item \codepath{main.cpp} e \codepath{src/satp/cli/BenchmarkCli.cpp}: entry point e loop
    comandi (\texttt{set}, \texttt{show}, \texttt{list}, \texttt{run}, \texttt{runstream},
    \texttt{runmerge}, \texttt{quit});
    \item \codepath{src/satp/cli/CliConfig.cpp}: parsing comandi, gestione \texttt{RunConfig},
    caricamento contesto runtime dal dataset (\texttt{sampleSize}, \texttt{runs}, \texttt{seed});
    \item \codepath{src/satp/cli/AlgorithmExecutor.cpp}: dispatch sugli algoritmi richiesti e
    instradamento verso le tre modalita' \texttt{RunMode::Normal}, \texttt{RunMode::Streaming},
    \texttt{RunMode::Merge};
    \item \codepath{src/satp/io/BinaryDatasetIO.h}: indicizzazione metadati binari,
    validazione tabella partizioni e loader per valori/bitset di verita';
    \item \codepath{src/satp/simulation/EvaluationFramework.h},
    \codepath{src/satp/simulation/EvaluationFramework.tpp},
    \codepath{src/satp/simulation/CsvResultWriter.h}: protocollo di valutazione e serializzazione
    risultati;
    \item \codepath{src/satp/algorithms/*.cpp}: implementazioni concrete degli sketch
    (HyperLogLog++, HyperLogLog, LogLog, ProbabilisticCounting);
    \item \codepath{scripts/generate_partitioned_dataset_bin.py}: generazione deterministica
    dataset partizionato compresso;
    \item \codepath{scripts/orchestrate_benchmarks.py}: campagna batch su griglie \((n,d,seed)\)
    e sweep parametrici standard o completi.
\end{itemize}

La pipeline end-to-end implementata e' la seguente:
\begin{enumerate}
    \item generazione del file \codepath{dataset_n_{n}_d_{d}_p_{p}_s_{seed}.bin}
    (\codepath{scripts/generate_partitioned_dataset_bin.py});
    \item parsing header/tabella con \codepath{satp::io::indexBinaryDataset}
    (\codepath{src/satp/io/BinaryDatasetIO.h});
    \item istanziazione della CLI (\codepath{main.cpp}) e dispatch in \codepath{AlgorithmExecutor};
    \item esecuzione \texttt{normal}/\texttt{streaming}/\texttt{merge} tramite
    \codepath{EvaluationFramework};
    \item scrittura CSV in \codepath{results/<algorithm>/<params>/results_*.csv}
    con \codepath{PathUtils::buildResultCsvPath} e \codepath{CsvResultWriter}.
\end{enumerate}

\section{Dataset binario compresso}

\subsection{Motivazione del formato}

Il formato binario partizionato evita la materializzazione in memoria
dell'intero dataset durante la valutazione. Ogni partizione e' compressa
indipendentemente con \texttt{zlib}; il framework carica solo il blocco
necessario alla run corrente tramite \codepath{BinaryDatasetPartitionReader}
(\codepath{src/satp/io/BinaryDatasetIO.h}).

La generazione e' implementata in \codepath{scripts/generate_partitioned_dataset_bin.py};
parsing, validazione e lettura sono implementati in
\codepath{src/satp/io/BinaryDatasetIO.h}.

\subsection{Struttura del file}

Il file contiene: header globale, tabella partizioni, payload compressi.
I parametri di formato sono codificati in
\codepath{src/satp/io/BinaryDatasetIO.h} e nello script generatore.

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{|l|l|}
        \hline
        Campo header & Significato \\
        \hline
        \texttt{MAGIC=SATPDBN2} & Identificatore formato \\
        \texttt{VERSION=2} & Versione schema \\
        \texttt{n} & Elementi per partizione \\
        \texttt{d} & Distinti per partizione \\
        \texttt{p} & Numero di partizioni \\
        \texttt{seed} & Seed globale del dataset \\
        \hline
    \end{tabular}
    \caption{Campi principali dell'header del dataset binario.}
    \label{tab:binary-header}
\end{table}

Ogni entry della tabella partizioni (\texttt{ENTRY\_SIZE=60}) contiene:
\begin{itemize}
    \item \texttt{values\_offset}, \texttt{values\_byte\_size};
    \item \texttt{truth\_offset}, \texttt{truth\_byte\_size};
    \item \texttt{n}, \texttt{d} locali (validati contro l'header globale);
    \item codifiche \texttt{values\_encoding} e \texttt{truth\_encoding},
    rispettivamente \texttt{ENCODING\_ZLIB\_U32\_LE} e
    \texttt{ENCODING\_ZLIB\_BITSET\_LE}.
\end{itemize}

\subsection{Bitset di verita' per \texorpdfstring{$F_0(t)$}{F0(t)}}

Per ogni partizione, oltre ai valori \texttt{uint32} compressi, viene salvato
un bitset compresso in cui il bit \(t\) (indice base zero) vale \(1\) se
l'elemento in posizione \(t\) e' una prima occorrenza nella partizione.
Nel framework streaming (\codepath{src/satp/simulation/EvaluationFramework.tpp})
la verita' prefissata e' ricostruita con accumulo:
\[
F_0(t)=\sum_{i=1}^{t} b_i.
\]
Questa scelta evita la ricostruzione di insiemi espliciti durante
la valutazione prefisso-per-prefisso.

\subsection{Indicizzazione e caricamento per partizione}

\codepath{indexBinaryDataset} valida magic, versione, consistenza dei metadati,
range degli offset e codifiche supportate; inoltre forza \texttt{d <= n}
(\codepath{src/satp/io/BinaryDatasetIO.h}).
\codepath{BinaryDatasetPartitionReader} mantiene un file handle aperto e fornisce:
\texttt{load(partitionIndex, outValues)} e
\texttt{loadWithTruthBits(partitionIndex, outValues, outTruthBits)}.
Il costo di memoria e' quindi proporzionale alla sola partizione corrente.

\section{Generazione dataset}

Lo script \codepath{scripts/generate_partitioned_dataset_bin.py} espone i
parametri \texttt{--n}, \texttt{--d}, \texttt{--p}, \texttt{--seed},
\texttt{--workers}, \texttt{--progress-batch}. I vincoli implementati sono:
\[
0 \le d \le n, \qquad d \le \min(n, 2^{32}), \qquad p > 0.
\]
La nomenclatura del file di output e' deterministica:
\codepath{dataset_n_{n}_d_{d}_p_{p}_s_{seed}.bin}.

Per ogni partizione, il generatore:
\begin{enumerate}
    \item deriva un seed locale da \((seed, partition\_index)\) tramite
    \codepath{_derive_partition_seed};
    \item estrae \(d\) ID distinti nel dominio \([0,\min(n,2^{32})-1]\);
    \item forza \(d\) posizioni per garantire almeno una occorrenza per ogni ID distinto;
    \item completa le altre posizioni campionando dagli ID distinti;
    \item costruisce il bitset di prime occorrenze e comprime separatamente valori e bitset.
\end{enumerate}

Quando \texttt{--workers>1}, la generazione e' parallelizzata per partizione con
\texttt{multiprocessing}; la barra di avanzamento e' emessa su \texttt{stderr}.

\section{Interfacce comuni e hashing}

\subsection{Interfaccia base degli algoritmi}

Il contratto comune è definito in \codepath{src/satp/algorithms/Algorithm.h}:
\begin{itemize}
    \item \texttt{process(uint32\_t id)}: aggiorna lo sketch;
    \item \texttt{count()}: restituisce la stima corrente di \(F_0\);
    \item \texttt{merge(const Algorithm\& other)}: combina due sketch compatibili;
    \item \texttt{reset()}: azzeramento stato;
    \item \texttt{getName()}: nome algoritmo.
\end{itemize}

La modalita' \texttt{runmerge} richiede in particolare la presenza di
\texttt{merge}; nel framework e' applicato un controllo di concetto
(\codepath{detail::MergeableAlgorithm} in
\codepath{src/satp/simulation/EvaluationFramework.tpp}).

\subsection{Hashing uniforme nel codice}

La randomizzazione è centralizzata in \codepath{src/satp/hashing.h}:
\begin{itemize}
    \item \texttt{splitmix64(uint64\_t)} come hash a 64 bit;
    \item \texttt{hash32\_from\_64(uint64\_t)} per derivare il dominio 32 bit.
\end{itemize}

HyperLogLog e LogLog applicano \texttt{hash32\_from\_64}; HyperLogLog++
usa direttamente il valore a 64 bit
(\codepath{src/satp/algorithms/HyperLogLog.cpp},
\codepath{src/satp/algorithms/LogLog.cpp},
\codepath{src/satp/algorithms/HyperLogLogPlusPlus.cpp}).

\section{Implementazione degli algoritmi}

\subsection{ProbabilisticCounting}

\texttt{ProbabilisticCounting} (\codepath{src/satp/algorithms/ProbabilisticCounting.cpp})
accetta \(\,L \in [1,31]\), altrimenti solleva \texttt{invalid\_argument}.
L'aggiornamento imposta il bit corrispondente alla posizione del primo 1 da
destra (\texttt{countr\_zero}) su hash troncato a \(L\) bit; la stima usa
l'indice del primo zero (\texttt{countr\_one}) e la costante \(\phi\) nel termine
\(\propto 2^R\) \cite{Flajolet_Martin_1985}.
L'operazione di merge e' un OR bit-a-bit con vincolo di compatibilita' su \(L\).

\subsection{LogLog}

\texttt{LogLog} (\codepath{src/satp/algorithms/LogLog.cpp}) implementa il
regime \emph{paper-strict} \cite{Durand_Flajolet_2003}:
\begin{itemize}
    \item \(k \in [4,16]\);
    \item \(L = 32\) obbligatorio.
\end{itemize}
Il merge richiede stesso \((k,L)\) e applica massimo componente per componente
sui registri.

\subsection{HyperLogLog}

\texttt{HyperLogLog} (\codepath{src/satp/algorithms/HyperLogLog.cpp}) impone
\(k \in [4,16]\) e \(L=32\). Il merge richiede uguaglianza di
\((k,L)\) e combina i registri con il massimo componente per componente.
La funzione \texttt{count()} applica tre regimi:
\begin{itemize}
    \item \emph{small-range} con linear counting;
    \item \emph{raw-range} con stima armonica;
    \item \emph{large-range} con correzione logaritmica su \(2^{32}\).
\end{itemize}
In accordo con \cite{Flajolet_Fusy_Gandouet_Meunier_2007}.

\subsection{HyperLogLog++}

\texttt{HyperLogLogPlusPlus}
(\codepath{src/satp/algorithms/HyperLogLogPlusPlus.cpp}) impone
\(p \in [4,18]\) e usa hash a 64 bit.
Lo stato alterna due formati:
\begin{itemize}
    \item \emph{sparse}: lista compressa di coppie codificate indice/\(\rho\);
    \item \emph{normal}: vettore registri denso.
\end{itemize}
Il passaggio \emph{sparse} \(\rightarrow\) \emph{normal} e' guidato dal confronto
tra costo della codifica sparsa (\texttt{sparseBits}) e costo denso
(\texttt{denseBits()}). La stima finale usa tabella di bias
(\codepath{hllpp_tables}) e soglia
\codepath{threshold_for_k(p)} per scelta tra linear counting e stima corretta.
Il merge richiede stesso \(p\).
Questa scelta segue l'impianto di \cite{Heule_Nunkesser_Hall_2013}.

\subsection{Domini parametrici enforced}

I domini accettati sono verificati in costruttore. I test in
\codepath{tests/algorithms/*.cpp} coprono esplicitamente i bound di HyperLogLog++,
HyperLogLog e LogLog; per ProbabilisticCounting coprono il comportamento di merge
e la compatibilita' dei parametri:
\begin{itemize}
    \item HyperLogLog++: \(p \in [4,18]\);
    \item HyperLogLog: \(k \in [4,16]\), \(L=32\);
    \item LogLog: \(k \in [4,16]\), \(L=32\);
    \item ProbabilisticCounting: \(L \in [1,31]\).
\end{itemize}
Lo script \codepath{scripts/orchestrate_benchmarks.py} replica gli stessi domini
nelle modalita' \texttt{--full} e \texttt{single-params}.

\section{Framework di valutazione}

\subsection{Modalita' \texttt{normal}, \texttt{streaming}, \texttt{merge}}

La classe \texttt{EvaluationFramework}
(\codepath{src/satp/simulation/EvaluationFramework.h}) espone tre percorsi:
\begin{itemize}
    \item \texttt{evaluateToCsv(...)} per endpoint per-partizione (\texttt{mode=normal});
    \item \texttt{evaluateStreamingToCsv(...)} per checkpoint prefissati
    (\texttt{mode=streaming});
    \item \texttt{evaluateMergePairsToCsv(...)} per confronto merge vs seriale su coppie
    \((0,1), (2,3), \ldots\) (\texttt{mode=merge}).
\end{itemize}
Nel backend binario, \texttt{runs} e \texttt{sample\_size} sono sempre derivati da
\codepath{datasetScope()} (metadati del dataset).

\subsection{Metriche implementate}

Le metriche sono aggregate da \codepath{ErrorAccumulator}
(\codepath{src/satp/simulation/ErrorAccumulator.h}) e includono:
\[
\text{mean},\ \text{variance},\ \text{stddev},\ \text{bias},\
\text{absolute\_bias},\ \text{relative\_bias},\
\text{mean\_relative\_error},\ \text{rmse},\ \text{mae},\
\text{rse\_observed},\ \text{truth\_mean}.
\]
In \codepath{AlgorithmExecutor.cpp} il valore teorico e' calcolato con
\(\mathrm{RSE}=1.04/\sqrt{m}\) (HyperLogLog e HyperLogLog++) e
\(\mathrm{RSE}=1.30/\sqrt{m}\) (LogLog); per
ProbabilisticCounting viene scritto \texttt{NaN}, in coerenza con
\cite{Flajolet_Fusy_Gandouet_Meunier_2007,Durand_Flajolet_2003}.

In streaming, i checkpoint sono generati da
\codepath{StreamingCheckpointBuilder::build} con strategia ibrida:
fase densa fino allo \(0.1\%\), fase logaritmica fino al \(10\%\), fase
logaritmica fino al \(100\%\), con massimo \(200\) checkpoint e ultimo punto
sempre \(t=n\)
(\codepath{src/satp/simulation/StreamingCheckpointBuilder.h}).

\subsection{Output CSV}

Le modalita' \texttt{normal} e \texttt{streaming} condividono lo schema:
\begin{center}
\small
\codepath{algorithm,params,mode,runs,sample_size,number_of_elements_processed,}\
\codepath{f0,seed,f0_mean_t,f0_heat_mean_t,variance,stddev,rse_theoretical,}\
\codepath{rse_observed,bias,absolute_bias,relative_bias,mean_relative_error,rmse,mae}
\end{center}
(\codepath{src/satp/simulation/CsvResultWriter.h}).

In \texttt{streaming}, \texttt{number\_of\_elements\_processed} coincide con il
checkpoint \(t\). La colonna \texttt{f0} contiene \(d\) globale del dataset,
mentre \texttt{f0\_mean\_t} contiene la verita' media al prefisso \(t\).

La modalita' \texttt{merge} usa invece:
\begin{center}
\small
\codepath{algorithm,params,mode,pairs,sample_size,pair_index,seed,estimate_merge,}\
\codepath{estimate_serial,delta_merge_serial_abs,delta_merge_serial_rel}
\end{center}
(\codepath{src/satp/simulation/CsvResultWriter.h}).

\section{CLI e orchestrazione sperimentale}

\subsection{CLI interattiva}

L'entry point \codepath{main.cpp} istanzia \codepath{BenchmarkCli}. I comandi
implementati in \codepath{src/satp/cli/BenchmarkCli.cpp} sono:
\texttt{help}, \texttt{show}, \texttt{list}, \texttt{set},
\texttt{run}, \texttt{runstream}, \texttt{runmerge}, \texttt{quit}.

\codepath{CliConfig::setParam} consente modifica dei parametri
\texttt{datasetPath}, \texttt{k}, \texttt{l}, \texttt{lLog}; il contesto runtime
(\texttt{sampleSize}, \texttt{runs}, \texttt{seed}) viene caricato dal dataset
con \codepath{loadDatasetRuntimeContext}
(\codepath{src/satp/cli/CliConfig.cpp}).

\codepath{PathUtils::buildResultCsvPath}
(\codepath{src/satp/cli/PathUtils.cpp}) costruisce i path:
\begin{itemize}
    \item \codepath{results/<AlgorithmName>/<params>/results_oneshot.csv};
    \item \codepath{results/<AlgorithmName>/<params>/results_streaming.csv};
    \item \codepath{results/<AlgorithmName>/<params>/results_merge.csv}.
\end{itemize}
La stringa \texttt{params} viene sanitizzata da
\codepath{sanitizeForPath} sostituendo i caratteri non alfanumerici con
\texttt{\_}.

\subsection{Orchestrazione batch}

Lo script \codepath{scripts/orchestrate_benchmarks.py} automatizza:
\begin{itemize}
    \item pianificazione della matrice \((n,d,seed)\) e numero partizioni \(p\);
    \item generazione dataset (opzionale) con \codepath{ensure_dataset};
    \item invio di payload CLI con sequenze \texttt{set} + \texttt{run}/\texttt{runstream}/\texttt{runmerge};
    \item sweep standard (parametri fissi) o completo (\texttt{--full}) su domini validi.
\end{itemize}

\section{Seed e riproducibilita'}

La propagazione del seed e' lineare:
\begin{enumerate}
    \item il generatore scrive \texttt{seed} nell'header binario
    (\codepath{struct.pack(HEADER_FMT, ..., seed_u64)} in
    \codepath{scripts/generate_partitioned_dataset_bin.py});
    \item \codepath{indexBinaryDataset} legge il campo e lo valida nel dominio
    \texttt{uint32} (\codepath{src/satp/io/BinaryDatasetIO.h});
    \item \codepath{CliConfig::loadDatasetRuntimeContext} trasferisce
    \texttt{seed} in \texttt{DatasetRuntimeContext}
    (\codepath{src/satp/cli/CliConfig.cpp});
    \item \codepath{EvaluationFramework} copia \texttt{binaryDataset.info.seed}
    nel membro \texttt{seed}
    (\codepath{src/satp/simulation/EvaluationFramework.cpp});
    \item \codepath{CsvResultWriter} serializza \texttt{seed} in tutte le righe
    \texttt{normal}/\texttt{streaming}/\texttt{merge}
    (\codepath{src/satp/simulation/CsvResultWriter.h}).
\end{enumerate}

Inoltre, la generazione usa \codepath{_derive_partition_seed(seed, partition_index)}
per rendere deterministico il contenuto di ogni partizione.

\section{Validazione e testing}

La suite principale e' in \codepath{tests/} e utilizza il dataset fisso
\codepath{tests/data/dataset_n_2000_d_1000_p_3_s_5489.bin}
(\codepath{tests/TestData.h}).

\subsection{Test algoritmici C++}

In \codepath{tests/algorithms} sono presenti test per:
\begin{itemize}
    \item smoke test di accuratezza per HLL, HLL++, LogLog e ProbabilisticCounting su dataset di riferimento;
    \item validazione domini parametrici su HLL, HLL++ e LogLog;
    \item proprieta' di merge (seriale/commutativita'/idempotenza), con controllo esatto per HLL, LogLog e ProbabilisticCounting e tolleranza relativa per HLL++;
    \item verifica di compatibilita' parametri in merge (eccezioni su mismatch).
\end{itemize}

\subsection{Test framework}

\codepath{tests/simulation/EvaluationFrameworkTest.cpp} verifica:
\begin{itemize}
    \item finitezza e segno non negativo delle metriche principali;
    \item in streaming con \texttt{NaiveCounting}, uguaglianza
    \(\hat{F}_0(t)=F_0(t)\) a ogni checkpoint;
    \item correttezza della policy checkpoint (inizio, monotonia, terminazione a \(n\));
    \item correttezza del percorso merge-pairs e del relativo CSV.
\end{itemize}

\section{Scelte progettuali e limiti attuali}

Le scelte implementative consolidate sono:
\begin{itemize}
    \item separazione netta tra algoritmi, I/O e framework di misura;
    \item formato binario compresso con loader per singola partizione;
    \item tre modalita' di run (\texttt{normal}, \texttt{streaming}, \texttt{merge});
    \item serializzazione CSV con schema stabile e colonna \texttt{seed} in ogni record.
\end{itemize}

Limiti correnti osservabili nel codice e nella documentazione:
\begin{itemize}
    \item assenza nel repository di implementazioni Count-Min Sketch, Bloom Filter e Ring Bloom Filter
    (cfr. TODO in \codepath{README.md});
    \item assenza di un test dedicato ai limiti di costruzione di ProbabilisticCounting (\(L \in [1,31]\));
    \item analisi empirica robusta multi-seed non ancora sistematizzata nel framework
    di reportistica.
\end{itemize}

\textbf{TODO (evidenza mancante).} Non e' ancora presente una valutazione
sistematica della sensibilita' al seed dell'hash su campagne multi-seed con
report dedicato.

\textbf{TODO (evidenza mancante).} Non e' ancora presente una campagna estesa
di degradazione dell'errore al crescere del numero di merge in catena
(oltre il confronto a coppie \((0,1),(2,3),\ldots\)).

\textbf{TODO (evidenza mancante).} Non e' ancora presente una misura empirica
strumentata della memoria reale per algoritmo durante run streaming/merge.

Nel Capitolo~\ref{chp:risultati} si analizzano i risultati sperimentali prodotti
dal framework descritto.
