%!TEX root = ../dissertation.tex

\chapter{Background}
\label{chp:background}
In questa tesi, il modello che rappresenta i dati in input, a differenza di algoritmi più tradizionali,
è chiamato \emph{modello di stream di dati} (data stream model). 
\section{Modello di stream di dati}
Nel modello di stream i dati \cite{Leskovec_Rajaraman_Ullman_2014} arrivano in modo continuo e potenzialmente
infinita. Rispetto l'utilizzo di un database tradizionale, non è possibile
accumulare tutto in memoria o su disco e interrogare i dati. 
Gli elementi devono essere processati al volo oppure vengono persi.

Inoltre, la velocità con cui i dati arrivano non è controllata dal sistema (più stream
possono arrivare a velocità e con formati diversi) e lo spazio di memoria
disponibile è limitato. Eventuali archivi storici possono esistere, ma non
sono pensati per rispondere a query online in tempi ragionevoli.

Iniziamo a definire formalmente gli elementi di una stream e come vengono processati.
\begin{definition}[Stream di dati]
Sia $\mathcal{U}$ un universo di chiavi. Senza perdita di generalità, assumiamo
$\mathcal{U} \subseteq \mathbb{N}$. Una \emph{stream di dati} è una sequenza ordinata
di elementi
\[
S = \langle x_1, x_2, \dots, x_s \rangle,
\]
dove ogni $x_i \in \mathcal{U}$ e $s$ può essere molto grande o non noto a priori.
\end{definition}

Per analizzare una stream è utile descriverla tramite le frequenze degli elementi.
\begin{definition}[Frequenze]
Data una stream $S$, la \emph{frequenza} di un elemento $a \in \mathcal{U}$ è
\[
f(a) = |\{ i \mid x_i = a \}|.
\]
La collezione delle frequenze può essere vista come un vettore $f \in \mathbb{N}^{|\mathcal{U}|}$.
\end{definition}

Una volta definita la nozione di frequenza, si specifica il modello di aggiornamento
con cui la stream viene osservata.
Nel modello della stream dei dati esistono diverse tipologie di modelli
\cite{Muthukrishnan_2005}. In questa tesi adottiamo il seguente.
\begin{definition}[Modello \emph{insertion-only}]
La stream è una sequenza di aggiornamenti del tipo $(a_t, \Delta_t)$, con
$a_t \in \mathcal{U}$ e $\Delta_t \ge 0$. Indichiamo con $A_t[j]$ la frequenza
dell'elemento $j$ dopo i primi $t$ aggiornamenti; allora
\[
A_t[j] =
\begin{cases}
A_{t-1}[j] + \Delta_t & \text{se } a_t = j,\\
A_{t-1}[j] & \text{altrimenti}.
\end{cases}
\]
\end{definition}
Se la stream è una lista di valori, ogni elemento $x_i$ può essere visto come
un aggiornamento $(x_i,1)$.

Esistono tuttavia modelli più generali. Nel \emph{turnstile} sono ammessi
anche aggiornamenti negativi, così che le frequenze possano aumentare o diminuire.
Nel modello a \emph{sliding window} si considerano solo gli ultimi $W$ aggiornamenti
della stream, scartando i più vecchi. Questi casi esulano dallo scopo della tesi,
ma sono citati per completezza.

Fissato il modello, l'obiettivo principale della tesi è stimare la cardinalità
dell'insieme dei distinti.
\begin{definition}[Numero di distinti]
Il \emph{numero di distinti} nella stream $S$ è
\[
F_0 = |\{ a \in \mathcal{U} \mid f(a) > 0 \}|.
\]
\end{definition}

Più in generale, il numero di distinti è un caso particolare di una famiglia
di misure note come \emph{frequency moments}.
\begin{definition}[Frequency moments]
Per ogni $k \ge 0$, il \emph{frequency moment} $F_k$ è definito come
\[
F_k = \sum_{a \in \mathcal{U}} f(a)^k.
\]
In particolare, $F_0$ corrisponde al numero di distinti.
\end{definition}

Per valutare la qualità di una stima si introduce la nozione di approssimazione
con parametri di accuratezza e confidenza.
\begin{definition}[$(\varepsilon,\delta)$-approssimazione]
Un algoritmo $A$ è detto \emph{$(\varepsilon,\delta)$-approssimante} per $F_0$ se, per ogni stream,
produce una stima $\tilde{F}_0$ tale che
\[
\Pr\bigl(|\tilde{F}_0 - F_0| \le \varepsilon F_0\bigr) \ge 1 - \delta,
\]
dove la probabilità è rispetto alla randomizzazione interna dell'algoritmo
\cite{BarYossef_Jayram_Kumar_Sivakumar_Trevisan_2002}.
\end{definition}
\paragraph{Esempio.}
Con $\varepsilon = 0{,}05$ e $\delta = 0{,}01$, l'algoritmo deve restituire una stima
entro il $5\%$ da $F_0$ con probabilità almeno $99\%$.

Supponiamo inoltre che l'universo abbia dimensione $n = |\mathcal{U}|$ e che ogni elemento $x_i$
richieda $b$ bit per essere rappresentato.

Le garanzie di accuratezza devono convivere con vincoli stringenti di tempo e memoria.
In questo contesto, un algoritmo di streaming deve:
\begin{itemize}
    \item processare ciascun elemento con costo $O(1)$ o quasi costante;
    \item usare memoria molto più piccola di $n$ e di $|\mathcal{U}|$;
    \item produrre una stima $\hat{F}_0$ con errore controllato, utilizzando $m$ bits, dove $m \ll n$.
\end{itemize}

Per rispettare questi vincoli si ricorre a funzioni hash che approssimano
una distribuzione uniforme sugli elementi e a strutture compatte, chiamate \textbf{sketch},
che riassumono le informazioni essenziali della stream senza conservarla esplicitamente.

Il vincolo più forte è quello di memoria: si richiede che lo spazio cresca
molto più lentamente della dimensione dell'universo.
\begin{definition}[Spazio sublineare]
Un algoritmo usa \emph{spazio sublineare} se la memoria $m$ cresce
asintoticamente meno di $n$, cioè $m = o(n)$, dove $n = |\mathcal{U}|$.
Si richiede che $m$ dipenda in modo polilogaritmico da $n$ e
polinomiale da $1/\varepsilon$ e $\log(1/\delta)$.
\end{definition}
\paragraph{Esempio.}
Per il problema dei distinti esistono algoritmi che ottengono una
$(1 \pm \varepsilon)$-approssimazione usando $O(\varepsilon^{-2} + \log n)$ bit
\cite{Kane_Nelson_Woodruff_2010}, che è molto meno dei $\Omega(n)$ bit necessari
per memorizzare l'insieme dei distinti.

\section{Algoritmi di streaming}
Il modello di data stream, con le caratteristiche appena descritte---flussi potenzialmente infiniti, velocità
non controllata e memoria limitata---rende inapplicabili gli approcci
classici basati su memorizzazione completa e analisi a posteriori.

Gli algoritmi di streaming nascono per produrre stime e
statistiche utili durante l'arrivo dei dati, lavorando in un solo passaggio e
mantenendo una sintesi compatta della stream.
\begin{definition}[Algoritmo di streaming]
Un algoritmo di streaming elabora una stream in un solo passaggio e mantiene
uno stato interno $M$ di dimensione limitata $m$. Per ogni elemento $x_i$ della
stream, lo stato viene aggiornato tramite una funzione
\[
M \leftarrow \mathrm{Update}(M, x_i),
\]
e in qualunque momento è possibile ottenere una risposta (o stima) tramite
\[
\hat{F}_0 \leftarrow \mathrm{Query}(M).
\]
Nel modello classico si richiede che $m$ sia sublineare rispetto a $n$ e che
il tempo per aggiornamento sia $O(1)$ o quasi costante \cite{Muthukrishnan_2005}.
\end{definition}

Dopo ogni aggiornamento, l'elemento appena osservato
può essere scartato: lo stato $M$ rappresenta una sintesi compatta dei dati,
spesso chiamata \emph{sketch} \cite{Cormode_2017}.

\subsubsection{Metriche di valutazioni} 
Nel modello classico, le prestazioni si
misurano in termini di \textbf{passaggi} sulla stream, \textbf{memoria} usata,
\textbf{tempo per elemento} e \textbf{accuratezza} della risposta. Per algoritmi
di approssimazione, l'accuratezza è espressa tramite un rapporto di
approssimazione e una probabilità di successo, spesso nel modello
$(\varepsilon,\delta)$ \cite{Prezza_2025}.

\subsubsection{Confronto con gli algoritmi online}
Gli algoritmi di streaming
sono affini agli algoritmi \emph{online}\cite{OnlineAlgorithms_Notes}, perché
operano senza disporre dell'intero input; tuttavia non sono identici, poiché nel
modello streaming è possibile talvolta differire l'azione fino all'arrivo di
piccoli blocchi di elementi, pur mantenendo una memoria molto limitata
\cite{Muthukrishnan_2005,Prezza_2025}. Un possibile esempio è fornito dagli
algoritmi per la \emph{sliding window}, che mantengono riassunti a blocchi per
stimare statistiche recenti con memoria limitata
\cite{Datar_Gionis_Indyk_Motwani_2002}. Nel nostro contesto, tutti gli algoritmi
implementati e trattati sono anche \emph{online}, perché processano ogni elemento
appena arriva, senza differire l'azione.

\subsubsection{Limitazioni di accuratezza rispetto allo spazio}
Per il problema dei calcolo degli elementi distinti, la
letteratura evidenzia un legame diretto tra accuratezza e spazio: ridurre
$\varepsilon$ implica un incremento della memoria necessaria. In particolare, si
considerano efficienti gli algoritmi che usano solo spazio polinomiale in
$1/\varepsilon$ e logaritmico nella lunghezza della stream e nella dimensione
dell'universo, con un costo per elemento molto basso
\cite{BarYossef_Jayram_Kumar_Sivakumar_Trevisan_2002}. Questo mette in evidenza
il \textbf{trade-off} centrale tra precisione e spazio necessario dello sketch.

\subsubsection{Randomizzazione e garanzie probabilistiche}
Questi algoritmi sono
spesso randomizzati: la probabilità nella definizione di $(\varepsilon,\delta)$
è rispetto alle scelte casuali interne dell'algoritmo e rappresenta una
garanzia probabilistica sulla qualità della stima
\cite{BarYossef_Jayram_Kumar_Sivakumar_Trevisan_2002}. Nelle implementazioni
pratiche, questa randomizzazione è tipicamente incarnata dalla funzione di hash,
assunta sufficientemente vicina a una scelta casuale (o parametrizzata da un
seed). La randomizzazione consente di ridurre drasticamente lo spazio rispetto
alle soluzioni deterministiche, a patto di accettare un errore controllato con
alta probabilità.

\subsubsection{Real-time vs offline}
Un'altra differenza rilevante rispetto
all'analisi tradizionale è la distinzione tra stime in tempo reale e analisi
\emph{offline}. Nei sistemi classici, gli aggiornamenti si registrano in un
archivio e le analisi complesse vengono svolte in \emph{warehouse}. Nel modello
di streaming, invece, molte applicazioni richiedono elaborazioni sofisticate in
quasi tempo reale, come rilevamento di anomalie, monitoraggio di trend o
cambiamenti improvvisi, e questo condiziona la progettazione degli algoritmi
\cite{Muthukrishnan_2005}.

\subsubsection{Mergeability}
In contesti distribuiti è spesso necessario combinare
riassunti di porzioni diverse della stream. Il concetto di \emph{mergeabilità}
formalizza la possibilità di unire due sintesi in una sintesi della loro unione
preservando le garanzie di errore e la dimensione dello stato: questo permette
di scalare gli algoritmi a scenari paralleli o gerarchici ed è una proprietà
centrale per gli sketch moderni \cite{Agarwal_Cormode_Huang_Phillips_Wei_Yi_2012}.

\section{Funzioni hash}
Qui si introduce il ruolo delle funzioni hash nella riduzione dello spazio e
nella randomizzazione delle stime. Da completare:
\begin{itemize}
    \item Definizione di hash e proprietà desiderate.
    \item Modello ideale: $h: \mathcal{U} \to [0,1)$ o $h:\mathcal{U}\to\{0,1\}^w$.
    \item Assunzioni tipiche: uniformità, indipendenza (o hash quasi-uniformi).
    \item Impatto di scelte non ideali sull'errore degli stimatori.
    \item Eventuale gestione del seed per riproducibilità.
\end{itemize}

\section{Sketch}
Si definisce lo sketch come struttura compatta che riassume la stream. Da completare:
\begin{itemize}
    \item Definizione generale di sketch e operazioni supportate (update, query).
    \item Dimensione dello sketch e relazione con i parametri ($k$, $m$, $L$).
    \item Esempi intuitivi (registri, bitmap, contatori).
    \item Due forme usate in tesi:
    \begin{itemize}
        \item bitmap / pattern di bit (Probabilistic Counting).
        \item registri e leading zeros (LogLog, HLL, HLL++).
    \end{itemize}
    \item (Cenno) serializzabilità e footprint in byte.
\end{itemize}

\section{Stimatori}
Si descrive cosa si intende per stimatore e in che senso la stima è corretta. Da completare:
\begin{itemize}
    \item Definizione di stimatore $\hat{F}_0$ e proprietà desiderate.
    \item Distinzione tra stimatore corretto (unbiased) e stimatore biased.
    \item Consistenza e varianza dello stimatore.
    \item Cenno a tecniche di bias correction.
\end{itemize}

\section{Metriche di errore}
Questa sezione introduce le metriche di qualità della stima. Da completare:
\begin{itemize}
    \item Errore assoluto e relativo.
    \item Bias e unbiased estimator.
    \item Varianza e standard error.
    \item RMSE e MAE come metriche aggregate.
    \item RSE teorica vs osservata (quando applicabile).
    \item Definizioni allineate alle colonne CSV del framework.
\end{itemize}

\section{Famiglia di algoritmi per count-distinct}
Sezione ponte (senza dettagli implementativi) per motivare il Capitolo 3. Da completare:
\begin{itemize}
    \item Baseline esatta vs sketch probabilistici.
    \item Linea FM/Probabilistic Counting $\rightarrow$ LogLog $\rightarrow$ HLL $\rightarrow$ HLL++.
    \item Differenze qualitative: riduzione varianza, correzioni di range, uso di registri.
\end{itemize}

\section{Spazio--accuratezza: ordini di grandezza}
Sezione opzionale ma utile per giustificare l'uso degli sketch. Da completare:
\begin{itemize}
    \item Dipendenza dello spazio da $(\varepsilon,\delta)$.
    \item Interpretazione di ``ottimalità'' a livello di ordine di grandezza.
\end{itemize}
