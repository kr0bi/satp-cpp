# Code Architecture Map

## 1. End-to-end pipeline (dataset generation -> loading -> evaluation -> CSV output)
- Datasets are generated by `scripts/generate_partitioned_dataset_bin.py` with parameters `n`, `d`, `p`, `seed`, and optional worker/progress knobs; the script enforces `0 ≤ d ≤ n` and `d ≤ 2^32`, builds partition fragments in parallel, compresses values as little-endian `uint32` blocks, records per-position “new distinct” bits, and writes the header + partition table that includes `seed` before copying the blobs into the final `dataset_n_{n}_d_{d}_p_{p}_s_{seed}.bin` file. (`scripts/generate_partitioned_dataset_bin.py:24-350`).
- Metadata about those binaries—magic/version, `n`, `d`, `p`, `seed`, zlib encodings, offsets—is parsed by `src/satp/io/BinaryDatasetIO.h` so `indexBinaryDataset` can validate every partition and expose the truth bitset loaders that `EvaluationFramework` relies on for streaming evaluation. (`src/satp/io/BinaryDatasetIO.h:105-330`).
- The CLI entry point `main.cpp` instantiates `BenchmarkCli`, which interprets commands like `set`, `run`, `runstream`, `runmerge`, and dispatches to `AlgorithmExecutor`; `CliConfig` parses user input, tracks `RunConfig` defaults, loads the dataset runtime context (sample size, runs, seed, repo root), and `PathUtils` sanitizes `params` strings and builds `results/<algorithm>/<params>/results_*.csv`. (`main.cpp:1-6`, `src/satp/cli/BenchmarkCli.cpp:8-69`, `src/satp/cli/CliConfig.cpp:27-111`, `src/satp/cli/PathUtils.cpp:6-72`).
- `AlgorithmExecutor` maps the CLI-selected algorithms to their specs, creates `EvaluationFramework` from the binary index, and calls `runSingleAlgorithm` for the three run modes. `runSingleAlgorithm` uses `EvaluationFramework::evaluateToCsv`, `evaluateStreamingToCsv`, or `evaluateMergePairsToCsv`, prints summaries, and ensures results directories exist before delegating to `CsvResultWriter`. (`src/satp/cli/AlgorithmExecutor.cpp:10-225`, `src/satp/simulation/EvaluationFramework.h:10-48`).
- `EvaluationFramework` streams partitions through `BinaryDatasetPartitionReader`, feeds values/truth bits into each algorithm, uses `ErrorAccumulator`/`StreamingCheckpointBuilder` to compute bias/variance/RMSE/RSE, and, for merging, processes partition pairs serially vs merged to track delta stats. (`src/satp/simulation/EvaluationFramework.tpp:150-312`, `src/satp/simulation/ErrorAccumulator.h:1-52`, `src/satp/simulation/StreamingCheckpointBuilder.h:11-82`).
- Every `evaluate*ToCsv` call writes metrics via `CsvResultWriter`, which prepends the header `algorithm,params,mode,...,mae` (normal/streaming) or the reduced merge header, and records the seed column for reproducibility before appending the stats row(s). (`src/satp/simulation/CsvResultWriter.h:16-175`).
- Tests in `tests/simulation/EvaluationFrameworkTest.cpp` cover the end-to-end flow (regular, streaming, merge, CSV) against the fixed `tests/data/dataset_n_2000_d_1000_p_3_s_5489.bin`, so they also document how the loader/truth bits/metrics interact. (`tests/simulation/EvaluationFrameworkTest.cpp:42-209`, `tests/TestData.h:1-24`).

## 2. Module map with file paths and responsibilities
- `scripts/generate_partitioned_dataset_bin.py`: deterministic dataset generation, per-partition zlib compression, truth bitset emission, filename convention, optional multiprocessing/progress, CLI entry for generation. (`scripts/generate_partitioned_dataset_bin.py:24-386`).
- `scripts/orchestrate_benchmarks.py`: dataset matrix planning (`n` × `d` × `seed`), optional dataset regeneration, results cleaning, standard vs full parameter sweeps, and automated serial interaction with the benchmark CLI via payloads that run `set` + `run/runstream/runmerge`. (`scripts/orchestrate_benchmarks.py:11-317`).
- `src/satp/io/BinaryDatasetIO.h`: dataset index reading (`n/d/p/seed`, encoding checks), partition loaders, truth-bit loaders, and the reusable `BinaryDatasetPartitionReader` used by the evaluation framework. (`src/satp/io/BinaryDatasetIO.h:105-330`).
- `src/satp/algorithms/`: concrete sketches with enforced domains and merge support: `HyperLogLogPlusPlus`, `HyperLogLog`, `LogLog`, `ProbabilisticCounting`, plus the `Algorithm` interface and auxiliary hashing utilities (`satp/hashing.h`). (`src/satp/algorithms/*.h/^.cpp`).
- `src/satp/simulation/`: evaluation framework (`EvaluationFramework.h/.cpp/.tpp`), progress helpers (`ProgressBar`), `CsvResultWriter`, error accumulation/streaming checkpoint builders, and `Loop` for replaying a dataset against a single algorithm (used by tests). (`src/satp/simulation/*`).
- `src/satp/cli/`: CLI glue—`BenchmarkCli`, `CliConfig`, `CliTypes`, `AlgorithmExecutor`, `PathUtils`—plus `RunConfig` defaults, `RunMode`, and dataset runtime context logic. (`src/satp/cli/*.cpp/h`).
- `tests/`: Catch2 suite covering algorithms and framework (allowed parameter ranges, merge properties, accuracy bands) plus reusable test dataset `tests/data/dataset_n_2000_d_1000_p_3_s_5489.bin`. (`tests/algorithms/*.cpp`, `tests/simulation/EvaluationFrameworkTest.cpp`, `tests/TestData.h`).

## 3. Entry points and runtime modes (run/runstream/runmerge etc)
- `main.cpp` → `BenchmarkCli`, which loops on standard commands; `set` mutates `RunConfig`, `show/list` report config/algorithms, and `run/runstream/runmerge` invoke `AlgorithmExecutor::run` with `RunMode::Normal/Streaming/Merge`. (`main.cpp:1-6`, `src/satp/cli/BenchmarkCli.cpp:8-69`, `src/satp/cli/CliTypes.h:11-40`).
- `AlgorithmExecutor` detects requested algorithms (or `all`), prints sample/runs/seed/context, and drives `EvaluationFramework` for the chosen mode, letting `runSingleAlgorithm` select the correct CSV writer and display for each mode. (`src/satp/cli/AlgorithmExecutor.cpp:10-225`).
- Automated orchestration uses `scripts/orchestrate_benchmarks.py` to feed payloads (`set datasetPath …`, `run…`) into the CLI binary, optionally covering streaming/merge/oneshot per algorithm. (`scripts/orchestrate_benchmarks.py:46-314`).

## 4. CSV columns currently produced and by which code paths
- Normal and streaming rows share the header emitted by `CsvResultWriter::writeHeaderIfNeeded`: `algorithm,params,mode,runs,sample_size,number_of_elements_processed,f0,seed,f0_mean_t,f0_heat_mean_t,variance,stddev,rse_theoretical,rse_observed,bias,absolute_bias,relative_bias,mean_relative_error,rmse,mae`. Each call to `appendNormal` or `appendStreaming` populates those columns with the algorithm label, params, runs/sample size from the dataset scope, truth mean, estimate mean, and the accumulated metrics (mean/variance/stddev/RSE/bias/MAE/RMSE). (`src/satp/simulation/CsvResultWriter.h:96-175`).
- Merge runs have their own header (`algorithm,params,mode,pairs,sample_size,pair_index,seed,estimate_merge,estimate_serial,delta_merge_serial_abs,delta_merge_serial_rel`) and write each pair’s delta stats. (`src/satp/simulation/CsvResultWriter.h:116-175`).
- All CSV records are emitted after `EvaluationFramework` produces `Stats`, `StreamingPointStats`, or `MergePairStats` and before `AlgorithmExecutor` prints the summary, so every output row carries the theoretical RSE from `rseHll`/`rseLogLog` or `NaN` if unknown. (`src/satp/cli/AlgorithmExecutor.cpp:37-169`, `src/satp/simulation/EvaluationFramework.tpp:223-312`).

## 5. Parameter domains actually enforced per algorithm
- HyperLogLog++ requires `p` (register index bits) in `[4,18]` and throws `invalid_argument` outside that range; it periodically flushes sparse buffers and eventually converts to the dense format that `merge` assumes matches `p`. (`src/satp/algorithms/HyperLogLogPlusPlus.cpp:13-148`).
- HyperLogLog and LogLog each set `k` in `[4,16]`, insist on `L = 32`, and recompute `m = 2^k`; merges require matching `(k,L)` before combining registers. (`src/satp/algorithms/HyperLogLog.cpp:12-124`, `src/satp/algorithms/LogLog.cpp:12-87`).
- ProbabilisticCounting accepts `L` in `[1,31]`; any other value throws `invalid_argument`, and merges require equal `L`. (`src/satp/algorithms/ProbabilisticCounting.cpp:10-57`).
- The CLI defaults come from `RunConfig` (`k=16`, `l=16`, `lLog=32`), and `scripts/orchestrate_benchmarks.py` documents the same domains plus the `full` sweep ranges (`HLL++ k=4..18`, `HLL k=4..16`, `LogLog k=4..16`, `PC L=1..31`). (`src/satp/cli/CliTypes.h:11-40`, `scripts/orchestrate_benchmarks.py:11-166`).
- `AlgorithmExecutor` computes theoretical RSE via `rseHll`/`rseLogLog` and leaves `rseUnknown()` (NaN) for ProbabilisticCounting, reinforcing the expectation that PC lacks a closed-form RSE. (`src/satp/cli/AlgorithmExecutor.cpp:37-49`).
- Algorithm tests assert the same domains (invalid arguments for out-of-range `k/L` and successful runs at the domain bounds), providing regression coverage for these restrictions. (`tests/algorithms/HyperLogLog.cpp:1-170`, `tests/algorithms/LogLogTest.cpp:1-87`, `tests/algorithms/ProbabilisticCountingTest.cpp:1-71`).

## 6. Seed usage path end-to-end
- The generator writes the requested `seed` into the header (`struct.pack(..., n, d, p, seed_u64)`) and derives per-partition seeds via `_derive_partition_seed` so the streams are deterministic. (`scripts/generate_partitioned_dataset_bin.py:60-350`).
- `CliConfig::loadDatasetRuntimeContext` loads `BinaryDatasetIndex`, reads `info.seed`, and stores it in `DatasetRuntimeContext`; the CLI prints it before running algorithms. (`src/satp/cli/CliConfig.cpp:73-111`).
- `EvaluationFramework` copies `binaryDataset.info.seed` into its `seed` field in the constructor and exposes `datasetScope()` for the sample size/runs metadata used throughout evaluation. (`src/satp/simulation/EvaluationFramework.cpp:6-25`).
- `CsvResultWriter` consistently writes the seed column for normal, streaming, and merge outputs, so every row can be tied back to the generator’s seed. (`src/satp/simulation/CsvResultWriter.h:16-175`).

## 7. Known limitations / TODO evidence from code
- The README’s TODO section explicitly lists gaps that still exist (JSON-based dataset refactor, merge/streaming workflows, merge operator fixes, plotting, Count-Min/Bloom/Ring Bloom sketches, instrumentation, distributed/serialization enhancements). (`README.md:96-179`).
